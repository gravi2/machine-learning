{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Learning Algorithm\n",
    "For solving the Continuous version of the Reacher environment, I decided to use the DDPG algorithm. As part of the DDPG implementation, I made use of:\n",
    "\n",
    " 1. Experience Replay ( Replay Buffer)\n",
    " 2. Actor Critic models\n",
    " 3. Soft update using local and target networks and controlling using hyperparameter TAU"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Hyperparameters\n",
    "I experiaemented with following Hyperparameters in various runs and solved the environment using the values shown below:\n",
    "\n",
    " 1. BATCH_SIZE = 1024 ( Size of the batch used for sampling)\n",
    " 2. BUFFER_SIZE = 1e5 ( Replay buffer size aka memory size )\n",
    " 3. TAU  = 0.1 ( To control how much the target network should be updated using the local network)\n",
    " 4. GAMMA = 0.99 ( discount factor )\n",
    " 5. ACTOR_LR = 1e-3 ( Learning rate used for actor model )\n",
    " 6. CRITIC_LR = 1e-3 ( Learning rate used for critic model )\n",
    " 7. UPDATE_EVERY = 100 (How frequent do we want to learn i.e at every UPDATE_EVERY steps)\n",
    " 8. LEARN_TIMES = 1 ( how many times should we perform learning during every UPDATE_EVERY steps)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Model Architecture\n",
    "My solution uses Actor Critic models to implement DDPG. As with any DDPG algorithm, the local actor critic models have their respective copy i.e target models. \n",
    "\n",
    "1. Actor Model - My actor model is made up of 3 fully connected layers ( 33x64, 64x256, 256 x action_size ). The activation function in the final output layer is tanh, as we need our action values to be between -1 and 1. The other two layers use relu activation functions. \n",
    "\n",
    "2. Critic model - My critic model is again made up of 3 fully connected layers ( 33x64, 64x256, 256x1). The last layer does not have any activation function. The other 2 layers use relu activation function. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Implementation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from solution import CollaborationSolution\n",
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "env = UnityEnvironment(file_name=\"./Tennis_Windows_x86_64/Tennis.exe\")\n",
    "\n",
    "try:\n",
    "    sol = CollaborationSolution(env,enable_wandb=True)\n",
    "    scores = sol.train(num_episodes=3500)\n",
    "    if scores:\n",
    "        # plot the scores\n",
    "        fig = plt.figure()\n",
    "        ax = fig.add_subplot(111)\n",
    "        plt.plot(np.arange(len(scores)), scores)\n",
    "        plt.ylabel('Score')\n",
    "        plt.xlabel('Episode #')\n",
    "        plt.show()\n",
    "finally:\n",
    "    env.close()\n"
   ]
  },
  {
   "source": [
    "# Graphs\n",
    "Here are few more graphs showing the learning scores per episode and average score for 100 episodes\n",
    "\n",
    "![Learning graph](./images/learning-graph.jpg)\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Try the trained model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from solution import ContinuousControlSolution\n",
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "\n",
    "env = UnityEnvironment(file_name='./Reacher_Windows_x86_64/Reacher.exe')\n",
    "\n",
    "try:\n",
    "    sol = ContinuousControlSolution(env,enable_wandb=False)\n",
    "    sol.watch_trained('checkpoint')\n",
    "finally:\n",
    "    env.close()\n"
   ]
  },
  {
   "source": [
    "# Video of the trained model\n",
    "%%HTML\n",
    "<div align=\"middle\">\n",
    "      <video width=\"80%\" controls>\n",
    "            <source src=\"./video.mp4\" type=\"video/mp4\">\n",
    "      </video>\n",
    "</div>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Ideas for future Work\n",
    "During this project, I was experience the unstable nature of DDPG. Based on my reading on DDPG, it seems like the algorithm is known to its unstable nature. Next I will like to try out algorithms like TRPO to solve this environment. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}
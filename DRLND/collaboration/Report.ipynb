{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Learning Algorithm\n",
    "For solving the Collaboration and Competition version of the Tennis environment, I decided to use the MADDPG algorithm. As part of the MADDPG implementation, I made use of:\n",
    "\n",
    " 1. Priority Experience Replay ( Replay Buffer)\n",
    " 2. Actor Critic models\n",
    " 3. Soft update using local and target networks and controlling using hyperparameter TAU"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Hyperparameters\n",
    "I experiaemented with following Hyperparameters in various runs and solved the environment using the values shown below:\n",
    "\n",
    " 1. BATCH_SIZE = 1024 ( Size of the batch used for sampling)\n",
    " 2. BUFFER_SIZE = 1e5 ( Replay buffer size aka memory size )\n",
    " 3. TAU  = 0.1 ( To control how much the target network should be updated using the local network)\n",
    " 4. GAMMA = 0.99 ( discount factor )\n",
    " 5. ACTOR_LR = 1e-3 ( Learning rate used for actor model )\n",
    " 6. CRITIC_LR = 1e-3 ( Learning rate used for critic model )\n",
    " 7. UPDATE_EVERY = 100 (How frequent do we want to learn i.e at every UPDATE_EVERY steps)\n",
    " 8. LEARN_TIMES = 1 ( how many times should we perform learning during every UPDATE_EVERY steps)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Model Architecture\n",
    "My solution uses Actor Critic models to implement MADDPG. As with any MADDPG algorithm, the local actor critic models have their respective copy i.e target models. \n",
    "\n",
    "1. Actor Model - My actor model is made up of 3 fully connected layers ( 48x256, 256x512, 512 x action_size ). The activation function in the final output layer is tanh, as we need our action values to be between -1 and 1. The other two layers use relu activation functions. \n",
    "\n",
    "2. Critic model - My critic model is again made up of 3 fully connected layers ( 48x256, 256+(action_size*2)x512, 512x1). The last layer does not have any activation function. The other 2 layers use relu activation function. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Implementation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "from solution import CollaborationSolution\n",
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "env = UnityEnvironment(file_name=\"./Tennis_Windows_x86_64/Tennis.exe\")\n",
    "\n",
    "try:\n",
    "    sol = CollaborationSolution(env,enable_wandb=True)\n",
    "    scores = sol.train(num_episodes=4000)\n",
    "    if scores:\n",
    "        # plot the scores\n",
    "        fig = plt.figure()\n",
    "        ax = fig.add_subplot(111)\n",
    "        plt.plot(np.arange(len(scores)), scores)\n",
    "        plt.ylabel('Score')\n",
    "        plt.xlabel('Episode #')\n",
    "        plt.show()\n",
    "finally:\n",
    "    env.close()\n"
   ]
  },
  {
   "source": [
    "# Graphs\n",
    "Here are few more graphs showing the learning scores per episode and average score for 100 episodes\n",
    "\n",
    "![Learning graph](./images/learning-graph.jpg)\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Try the trained model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from solution import CollaborationSolution\n",
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "\n",
    "env = UnityEnvironment(file_name=\"./Tennis_Windows_x86_64/Tennis.exe\")\n",
    "\n",
    "try:\n",
    "    sol = CollaborationSolution(env,enable_wandb=False)\n",
    "    sol.watch_trained('./checkpoints/checkpoint-0.093000001385808')\n",
    "finally:\n",
    "    env.close()\n"
   ]
  },
  {
   "source": [
    "# Video of the trained model\n",
    "%%HTML\n",
    "<div align=\"middle\">\n",
    "      <video width=\"80%\" controls>\n",
    "            <source src=\"./video.mp4\" type=\"video/mp4\">\n",
    "      </video>\n",
    "</div>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Ideas for future Work\n",
    "This project was a tricky one. It look me many tries to figure out Multi Agent setup for DDPG. The learning process also currently requires 5+ learnings per episode. This is slowing down the learning. \n",
    "\n",
    "The learning was also not statble at times. Few things for next improvements:\n",
    "\n",
    "1. The MADDPG paper talks about few improvements to MADDPG like using Policy Ensembles and Inferring policies of other Agents. I will like to explore these further. \n",
    "2. I will like to try out other algorithms like TRPO in MA to solve this environment. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}